# Building-a-miniature-GPT
This project involves building a miniature Generative Pretrained Transformer (MiniGPT) from scratch using Python and PyTorch. The goal is to understand and implement the core components of transformer-based language models, including tokenization, embedding layers, self-attention mechanisms, and autoregressive training. By training the model on a small text dataset, the project demonstrates how large language models like GPT work at a fundamental level. This hands-on approach offers a deep dive into model architecture, training dynamics, and inference processes in modern natural language processing.

**Dataset Link:** https://github.com/taivop/joke-dataset/blob/master/reddit_jokes.json

**References:**  https://jalammar.github.io/illustrated-transformer/

![20250605_1841_AI Scale Contrast_simple_compose_01jx03bx16ejbtx6fegpy6hmsw](https://github.com/user-attachments/assets/8e546a37-c77d-43c1-b5a8-dbbbc2dd176e)

